{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3b2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435327a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data):\n",
    "    \"\"\"\n",
    "    Inspect the structure of the 'data' dictionary.\n",
    "    For each year, it prints the available keys, types, and shapes if applicable.\n",
    "    \"\"\"\n",
    "    print(\"üîé Exploring data structure\\n\" + \"-\"*60)\n",
    "    for year_key, year_data in data.items():\n",
    "        print(f\"{year_key}\")\n",
    "        \n",
    "        if not isinstance(year_data, dict):\n",
    "            print(f\"  ‚ö†Ô∏è Expected dict, got {type(year_data)}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Loop through subkeys (like imgs_array, iris_index, etc.)\n",
    "        for key, value in year_data.items():\n",
    "            info = f\"  ‚îú‚îÄ {key:<25} ‚Üí \"\n",
    "            \n",
    "            # Describe arrays\n",
    "            if isinstance(value, (list, tuple)):\n",
    "                info += f\"list[{len(value)}]\"\n",
    "            elif isinstance(value, dict):\n",
    "                info += f\"dict[{len(value)}]\"\n",
    "            elif hasattr(value, \"shape\"):\n",
    "                info += f\"array shape={value.shape}, dtype={getattr(value, 'dtype', 'N/A')}\"\n",
    "            else:\n",
    "                info += str(type(value))\n",
    "            \n",
    "            print(info)\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "\n",
    "def save(data, save_path):\n",
    "    with open(save_path, \"wb\") as f:\n",
    "       pickle.dump(data, f)\n",
    "    print(f\"‚úÖ Data saved successfully at: {save_path}\")\n",
    "\n",
    "def load(save_path):\n",
    "    with open(save_path, \"rb\") as f:\n",
    "       data = pickle.load(f)\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "    return data\n",
    "    \n",
    "def read_band(file):\n",
    "    \"\"\"Lit une bande Landsat et nettoie les valeurs aberrantes.\"\"\"\n",
    "    with rasterio.open(file) as src:\n",
    "        band = src.read(1).astype(float)\n",
    "        # Supprime les valeurs satur√©es ou nulles\n",
    "        band[(band <= 0) | (band >= 60000)] = 0\n",
    "        return band\n",
    "    \n",
    "def get_array_img(path, date):\n",
    "    B1 = \"\"\n",
    "    B2 = \"\"\n",
    "    B3 = \"\"\n",
    "    B4 = \"\"\n",
    "    B5 = \"\"\n",
    "    B6 = \"\"\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            if f.lower().endswith(\"b1.tif\"):\n",
    "                B1 = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\"b2.tif\"):\n",
    "                B2 = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\"b3.tif\"):\n",
    "                B3 = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\"b4.tif\"):\n",
    "                B4 = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\"b5.tif\"):\n",
    "                B5 = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\"b6.tif\"):\n",
    "                B6 = os.path.join(dirpath, f)\n",
    "    \n",
    "\n",
    "    if date >= 2013:\n",
    "        red = read_band(B6) \n",
    "        green = read_band(B5) \n",
    "        blue = read_band(B4)\n",
    "\n",
    "        red = red / red.max()\n",
    "        green = green / green.max()\n",
    "        blue = blue / blue.max()\n",
    "    \n",
    "    else:\n",
    "        red = read_band(B5)\n",
    "        green = read_band(B4)\n",
    "        blue = read_band(B3)\n",
    "\n",
    "    \n",
    "    rgb = np.dstack((red, green, blue))\n",
    "    p2, p98 = np.nanpercentile(rgb, (2, 98))\n",
    "    rgb_norm = np.clip((rgb - p2) / (p98 - p2), 0, 1)\n",
    "    rgb_8bit = (rgb_norm * 255).astype(np.uint8)\n",
    "\n",
    "    return rgb_8bit\n",
    "\n",
    "def return_tif_files(dir):\n",
    "    for dirpath, _, filenames in os.walk(dir):\n",
    "        for f in filenames:\n",
    "            if f.lower().endswith(\".tif\"):\n",
    "                path = os.path.join(dirpath, f)\n",
    "                try:\n",
    "                    with rasterio.open(path) as src:\n",
    "                        if src.crs is not None and src.transform is not None:\n",
    "                            return path\n",
    "                \n",
    "                except rasterio.errors.RasterioIOError:\n",
    "                    continue\n",
    "\n",
    "def get_grid_coordinates(path, format = \"lat_lon\"): #this file ends with .Tif\n",
    "\n",
    "    with rasterio.open(path) as src:\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        width, height = src.width, src.height\n",
    "\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "\n",
    "    xs = np.array(xs).reshape(height, width)\n",
    "    ys = np.array(ys).reshape(height, width)\n",
    "\n",
    "    if format != \"lat_lon\":\n",
    "        return xs, ys\n",
    "    \n",
    "    transformer = Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
    "    lons, lats = transformer.transform(xs, ys)\n",
    "\n",
    "    return lons, lats\n",
    "\n",
    "def world_to_pixel(x, y, transform):\n",
    "    \"\"\"Convert geospatial coords (x,y) to pixel indices (row,col).\"\"\"\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(round(row)), int(round(col))\n",
    "\n",
    "def pixel_to_world(col, row, transform):\n",
    "    \"\"\"Convert pixel indices (col,row) to geospatial coordinates (x,y).\"\"\"\n",
    "    x, y = transform * (col, row)\n",
    "    return x, y\n",
    "\n",
    "def get_city_patch_params(city: str):\n",
    "    \"\"\"\n",
    "    Returns i_start, j_start, i_length, j_length for a given city.\n",
    "    \"\"\"\n",
    "\n",
    "    if city == \"Lyon\":\n",
    "        i_start, j_start = 4400, 5000\n",
    "        i_length, j_length = 1000, 800\n",
    "\n",
    "    elif city == \"Paris\":\n",
    "        i_start, j_start = 3000, 3000\n",
    "        i_length, j_length = 1600, 1600\n",
    "\n",
    "    elif city == \"Toulouse\":\n",
    "        i_start, j_start = 1800, 2600\n",
    "        i_length, j_length = 1000, 600\n",
    "\n",
    "    elif city == \"Bordeaux\":\n",
    "        i_start, j_start = 2600, 4100\n",
    "        i_length, j_length = 800, 800\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown city: {city}\")\n",
    "\n",
    "    return i_start, j_start, i_length, j_length\n",
    "\n",
    "def get_geospatial_coordinates(path, num_patch, i_start, j_start, i_length, j_length):\n",
    "\n",
    "    \n",
    "    path2021 = os.path.join(path, \"2021\")\n",
    "    tif_2021 = return_tif_files(path2021)\n",
    "    with rasterio.open(tif_2021) as src2021:\n",
    "        transform2021 = src2021.transform\n",
    "    \n",
    "    i_s = np.random.randint(i_start, i_start+i_length, num_patch)\n",
    "    j_s = np.random.randint(j_start, j_start+j_length, num_patch)\n",
    "\n",
    "    x_s = [0]* num_patch\n",
    "    y_s = [0]* num_patch\n",
    "\n",
    "    for k in range(num_patch):\n",
    "        i, j = i_s[k], j_s[k]\n",
    "        x, y = pixel_to_world(j, i, transform2021)\n",
    "        x_s[k], y_s[k] = x, y\n",
    "\n",
    "    return x_s , y_s\n",
    "\n",
    "def get_patches(x_s, y_s, path, patch_size):\n",
    "    num_patches = len(x_s)\n",
    "    data = {}\n",
    "\n",
    "    for date in range(2013, 2022):\n",
    "        if date != 2012:\n",
    "            print(date)\n",
    "            new_path = os.path.join(path, str(date))\n",
    "            path_tif_file = return_tif_files(new_path)\n",
    "            img = get_array_img(new_path, date)\n",
    "            imgs_array = np.zeros((num_patches, patch_size, patch_size, 3), dtype=np.uint8)\n",
    "            space_coordinates = np.zeros((num_patches, patch_size, patch_size, 2), dtype=np.float32)\n",
    "\n",
    "            with rasterio.open(path_tif_file) as src:\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "\n",
    "            # prepare coordinate transformer (to lat/lon)\n",
    "            transformer = Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "            for k in range(num_patches):\n",
    "                x, y = x_s[k], y_s[k]\n",
    "                i, j = world_to_pixel(x, y, transform)\n",
    "\n",
    "                # image patch\n",
    "                imgs_array[k, :, :, :] = img[i:i+patch_size, j:j+patch_size, :]\n",
    "\n",
    "                # grid of pixel indices within the patch\n",
    "                rows = np.arange(i, i + patch_size)\n",
    "                cols = np.arange(j, j + patch_size)\n",
    "                cols_grid, rows_grid = np.meshgrid(cols, rows)\n",
    "\n",
    "                # convert to map coordinates\n",
    "                xs, ys = rasterio.transform.xy(transform, rows_grid, cols_grid, offset=\"center\")\n",
    "                xs = np.array(xs).reshape((patch_size, patch_size))\n",
    "                ys = np.array(ys).reshape((patch_size, patch_size))\n",
    "\n",
    "                # convert map coords ‚Üí lon/lat\n",
    "                lons, lats = transformer.transform(xs, ys)\n",
    "\n",
    "                # stack and save\n",
    "                space_coordinates[k, :, :, :] = np.dstack((lats, lons)).astype(np.float32)\n",
    "\n",
    "            data[str(date)] = {\n",
    "                \"imgs_array\": imgs_array,\n",
    "                \"space_coordinates\": space_coordinates\n",
    "            }\n",
    "\n",
    "    return data\n",
    "\n",
    "def assign_iris_to_data(data, iris_csv_paths, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    Adds 'iris_index' arrays to the existing 'data' dictionary.\n",
    "    Each iris_index has shape [num_patches, patch_size, patch_size],\n",
    "    with each pixel containing its CODE_IRIS (string or np.nan).\n",
    "    \"\"\"\n",
    "\n",
    "    for year, content in data.items():\n",
    "        if not year.isdigit():\n",
    "            continue\n",
    "        if int(year) not in iris_csv_paths:\n",
    "            print(f\"‚ö†Ô∏è No IRIS CSV for {year}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüìÖ Processing year {year}...\")\n",
    "\n",
    "        # Load IRIS polygons for that year\n",
    "        iris_csv = iris_csv_paths[int(year)]\n",
    "        iris_df = pd.read_csv(iris_csv)\n",
    "        iris_df[\"geometry\"] = iris_df[\"geometry\"].apply(wkt.loads)\n",
    "        iris_gdf = gpd.GeoDataFrame(iris_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "        # Get pixel coordinate array\n",
    "        coords = content[\"space_coordinates\"]  # shape [num_patches, H, W, 2]\n",
    "        num_patches, H, W, _ = coords.shape\n",
    "        total_points = num_patches * H * W\n",
    "\n",
    "        print(f\"   ‚Üí Flattening {total_points:,} pixels...\")\n",
    "\n",
    "        # Flatten coordinates\n",
    "        flat_coords = coords.reshape(-1, 2)\n",
    "        lats = flat_coords[:, 0]\n",
    "        lons = flat_coords[:, 1]\n",
    "\n",
    "        # Prepare output\n",
    "        codes = np.empty(total_points, dtype=object)\n",
    "\n",
    "        # Process in batches\n",
    "        for start in tqdm(range(0, total_points, batch_size), desc=f\"Joining {year}\", ncols=80):\n",
    "            end = min(start + batch_size, total_points)\n",
    "            batch_lats = lats[start:end]\n",
    "            batch_lons = lons[start:end]\n",
    "\n",
    "            # Create GeoDataFrame of points\n",
    "            points_gdf = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(batch_lons, batch_lats),\n",
    "                crs=\"EPSG:4326\"\n",
    "            )\n",
    "\n",
    "            # Spatial join (vectorized)\n",
    "            joined = gpd.sjoin(\n",
    "                points_gdf,\n",
    "                iris_gdf[[\"CODE_IRIS\", \"geometry\"]],\n",
    "                how=\"left\",\n",
    "                predicate=\"within\"\n",
    "            )\n",
    "\n",
    "            codes[start:end] = joined[\"CODE_IRIS\"].to_numpy()\n",
    "\n",
    "        # Reshape back to [num_patches, H, W]\n",
    "        iris_index = codes.reshape(num_patches, H, W)\n",
    "\n",
    "        # Attach to data\n",
    "        data[year][\"iris_index\"] = iris_index\n",
    "\n",
    "        matched = np.count_nonzero(~pd.isna(codes))\n",
    "        print(f\"‚úÖ Finished {year}: matched {matched:,}/{total_points:,} pixels\")\n",
    "\n",
    "    print(\"\\n‚úÖ Added 'iris_index' to all matching years.\")\n",
    "    return data\n",
    "\n",
    "iris_csv_paths = {\n",
    "    2011: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2011\\iris_latlon.csv\",\n",
    "    2012: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2012\\iris_latlon.csv\",\n",
    "    2013: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2013\\iris_latlon.csv\",\n",
    "    2014: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2014\\iris_latlon.csv\",\n",
    "    2015: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2015\\iris_latlon.csv\",\n",
    "    2016: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2016\\iris_latlon.csv\",\n",
    "    2017: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2017\\iris_latlon.csv\",\n",
    "    2018: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2018\\iris_latlon.csv\",\n",
    "    2019: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2019\\iris_latlon.csv\",\n",
    "    2020: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2020\\iris_latlon.csv\",\n",
    "    2021: r\"C:\\Users\\adamh\\Desktop\\IRIS\\2021\\iris_latlon.csv\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910c659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Toulouse\"\n",
    "base_dir = r\"C:\\Users\\adamh\\Desktop\\Satelite_images\"\n",
    "\n",
    "save_path = os.path.join(base_dir, f\"{city}_data.pkl\")\n",
    "city_path = os.path.join(base_dir, \"Satellite_Images\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e4d8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n"
     ]
    }
   ],
   "source": [
    "num_patch = 500\n",
    "i_start, j_start, i_length, j_length = get_city_patch_params(city)\n",
    "patch_size = 128\n",
    "\n",
    "x_s, y_s = get_geospatial_coordinates(city_path, num_patch, i_start, j_start, i_length, j_length)\n",
    "data = get_patches(x_s, y_s, city_path, patch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8942fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Processing year 2013...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2013: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:10<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2013: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2014...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2014: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2014: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2015...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2015: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2015: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2016...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2016: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2016: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2017...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2017: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2017: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2018...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2018: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2018: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2019...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2019: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2019: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2020...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2020: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:11<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2020: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "üìÖ Processing year 2021...\n",
      "   ‚Üí Flattening 8,192,000 pixels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining 2021: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished 2021: matched 8,192,000/8,192,000 pixels\n",
      "\n",
      "‚úÖ Added 'iris_index' to all matching years.\n",
      "‚úÖ Data saved successfully at: C:\\Users\\adamh\\Desktop\\Satelite_images\\Toulouse_data.pkl\n"
     ]
    }
   ],
   "source": [
    "data = assign_iris_to_data(data, iris_csv_paths)\n",
    "save(data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a16879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
